{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 4: \\\n",
    "**Detekcja obiektów z wykorzystaniem algorytmu YOLO**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "10.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Detekcją obiektów** nazywamy problem określenia i zlokalizowania poszczególnych rzeczy, które znajdują się na analizowanym obrazie. W rzeczywistości, podczas detekcji obiektów rozwiązuje się dwa problemy:\n",
    "* klasyfikacji -  przypisania obiektu do jednej ze znanych klas (tj. odpowiedź na pytanie, czym jest odnaleziony obiekt),\n",
    "* regresji - oszacowanie położenia prostokąta otaczającego odnaleziony obiekt (tj. odpowiedź na pytanie, gdzie na obrazku znajduje się odnaleziony\n",
    "obiekt). Prostokąt ten nazywamy często *bounding box*em.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/Argenni/GUiAO_lab/main/rys/06_object_detection.png'/>\n",
    "\n",
    "<font size=\"1\">Grafika: baeldung.com</font>\n",
    "</div>\n",
    "\n",
    "Najpopularniejsze metody detekcji obiektów to:\n",
    "* **Region-based Convolutional Networks** (R-CNN) - których działanie polega na tym, że w trakcie analizy obrazka wyznaczane są tzw. *regions of interest*, tj. obszary, w których potencjalnie może znajdować się znaczący obiekt (RoI może być wyznaczane z pomocą heurystycznych zasad/modeli, bądź też konwolucyjnych sieci neuronowych), a następnie obszary te (lub też wyodrębione z nich cechy) przepuszczane są przez CNN, który ma ocenić, co się na nim znajduje i gdzie dokładnie. \n",
    "* **Algorytm YOLO** (ang *You Only Look Once*) - model oparty na CNN, który nie analizuje każdego RoI z osobna, lecz wystarczy mu jeden *forward pass*, aby dokonać detekcji znajdujących się na obrazku obiektów (co oszczędza czas i moc obliczeniową): obraz dzielony jest na mniejsze części z wykorzystaniem siatki a każda komórka takiej siatki pozwala na wykrycie 1 obiektu.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/Argenni/GUiAO_lab/main/rys/07_yolo.png'/>\n",
    "\n",
    "<font size=\"1\">Grafika: baeldung.com</font>\n",
    "</div>\n",
    "\n",
    "Podczas detekcji zwracane są takie dane o wykrytym obiekcie, jak:\n",
    "* indeks $c$ klasy, do której należy wykryty obiekt,\n",
    "* prawdopodobieństwo $p_c$ przynależenia tego obiektu do tejże klasy,\n",
    "* położenie ($B_x,B_y$) środka otaczającego obiekt *bounding box*u,\n",
    "* wymiary (szerokość $B_w$ x wysokość $B_h$) otaczającego obiekt *bounding box*u.\n",
    "\n",
    "\n",
    "\n",
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z dokonywaniem detekcji obiektów z wykorzystaniem algorytmu YOLOv3. W trakcie ćwiczenia użyjemy gotowy, wytrenowany model znajdujący się w bibliotece OpenCV, więc nie będzie konieczności implementowania samego procesu trenowania naszego modelu; w to miejsce skupimy się bardziej na analizie danych, które zwraca taki model i właściwej ich interpretacji (wraz z wyrysowaniem oszacowanych *bounding box*ów naokoło wykrytych przez model obiektów oraz ich filtracji).\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## Detekcja obrazów z wykorzystaniem pre-trenowanego modelu YOLOv3 z biblioteki OpenCV\n",
    "\n",
    "W ramach tego ćwiczenia, silnie opierać się będziemy na [TYM](https://towardsdatascience.com/yolo-object-detection-with-opencv-and-python-21e50ac599e9) tutorialu.\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic),\n",
    "* **Matplotlib** - biblioteka wspomagająca wizualizację pracy czy analizę danych poprzez wyświetlanie wykresów,\n",
    "* **OpenCV** - biblioteka zawierająca gotowe implementacje wielu algorytmów związanych z analizą obrazów (*Computer Vision*),\n",
    "* **wget** - biblioteka umożliwiająca pobieranie plików z zewnętrznych źródeł (np. stron www), **os** - biblioteka umożliwiająca zarządzanie tymi plikami z poziomu systemu operacyjnego, oraz **copy** - biblioteka umożliwiająca tworzenie \"twardych\" kopii danych podczas ich przekazywania do różnych metod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install numpy==1.22.3\n",
    "! python -m pip install matplotlib==3.4.2\n",
    "! python -m pip install opencv-python=4.8.1.78\n",
    "! python -m pip install wget==3.2\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(10) \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import wget\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie i przygotowanie danych\n",
    "\n",
    "Na wstępie musimy przygotować pliki, z których będziemy korzystać w trakcie wykonywania tego ćwiczenia. Potrzebujemy:\n",
    "* pliku `yolov3.cfg`, zawierającą konfigurację naszego modelu, tak, by mógł on być obsłużony przez OpenCV,\n",
    "* pliku `yolov3.weights`, który zawiera macierze wag naszej sieci neuronowej (już po jej treningu),\n",
    "* pliku z obrazem, który będziemy analizować - niech będzie to wykorzystywany już wcześniej plik `dog.jpg` przedstawiający psa.\n",
    "Ponieważ YOLO w wersji opracowanej w OpenCV wytrenowany został z wykorzystaniem zestawu danych COCO, potrzebujemy też plik `coco_classes.txt`, zawierający listę kategorii (klas), które rozróżnialne są w ramach tego zestawu danych.\n",
    "\n",
    "Uruchom kod z poniższej komórki, aby:\n",
    "* ściągnąć i załadować do *workspace* odpowiednie pliki,\n",
    "* wygenerować (stałe w ramach całego skrytpu) kolory RGB, jakimi będą oznaczane wykryte obiekty kolejnych klas - zmienna `colors`,\n",
    "* z wykorzystaniem metody `cv.dnn.readNet` ([TUTAJ](https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422)) wczytać do zmiennej `yolo` wytrenowany model YOLOv3 (metoda ta pootrzebuje ścieżek do pliku z konfiguracją i wagami modelu YOLO), a także zidentyfikować wyjściowe warstwy modelu (te zwracające interesujące nas predykcje), tak, by zbierać odpowiedzi tylko od nich,\n",
    "* wczytać, wyświetlić oraz odpowiednio przekształcić zdjęcie do analizy (zdjęcie normalizujemy i przekształcamy do wymiaru 416 x 416) - tak przygotowane zdjęcie zapisujemy jako `input_img`. <font size=\"2\">Ciekawostka: aby poprawnie wyświelić obrazki wczytane przez OpenCV za pomocą `imshow` z biblioteki Matplotlib, należy zamienić kolejność kodowanych kolorów: OpenCV używa BGR, a Matplotlib klasycznego RGB.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Inicjalizacja -----------------------\n",
    "# Pobierz niezbędne pliki\n",
    "if not os.path.exists(\"utils/yolov3.cfg\"): # plik z konfiguracją modelu YOLO\n",
    "    wget.download(\"https://raw.githubusercontent.com/arunponnusamy/object-detection-opencv/master/yolov3.cfg\", out=\"utils/yolov3.cfg\")\n",
    "if not os.path.exists(\"utils/yolov3.weights\"): # plik z wagami wytrenowanego modelu YOLO\n",
    "    wget.download(\"https://pjreddie.com/media/files/yolov3.weights\", out=\"utils/yolov3.weights\")\n",
    "if not os.path.exists(\"utils/coco_classes.txt\"): # plik z klasami w zestawie danych COCO\n",
    "    wget.download(\"https://raw.githubusercontent.com/arunponnusamy/object-detection-opencv/master/yolov3.txt\", out=\"utils/coco_classes.txt\")\n",
    "with open(\"utils/coco_classes.txt\", \"r\") as f:\n",
    "    classes = [s.strip() for s in f.readlines()]\n",
    "# Zmienna przechowująca kolory boxów dla każdej z klas\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "if not os.path.exists(\"utils/dog.jpg\"): # plik z przykładowym zdjęciem psa\n",
    "    wget.download(\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", out=\"utils/dog.jpg\")\n",
    "# Wczytaj model i wyodrębnij jego wyjściowe warstwy\n",
    "yolo = cv2.dnn.readNet(\"utils/yolov3.weights\", \"utils/yolov3.cfg\")\n",
    "layers = yolo.getLayerNames()\n",
    "output_layers = [layers[i - 1] for i in yolo.getUnconnectedOutLayers()]\n",
    "# Wczytaj i przetwórz obraz\n",
    "image = cv2.imread(\"utils/dog.jpg\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "img_dim = (image.shape[1], image.shape[0])\n",
    "input_img = cv2.dnn.blobFromImage(\n",
    "    image=image, \n",
    "    scalefactor=1/255, \n",
    "    size=(416,416), \n",
    "    mean=(0,0,0), \n",
    "    swapRB=True, \n",
    "    crop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja wykonana przez YOLO i interpretacja zwróconych wartości\n",
    "\n",
    "Używany przez nas model YOLO posiada 3 warstwy wyjściowe - każda różni się ilością komórek, na jakie dzieli obraz, a zatem też ilością detekcji, które mogą dokonać. Jeśli jako argument dla metody `model.setInput()` podamy zmienną z przekształconym odpowiednio obrazem, a jako argument dla `model.forward()` podamy nazwy warstw wyjściowych, otrzymanych w wyniku `model.getUnconnectedOutLayers()`, uzyskamy odpowiedź naszego modelu z dokonanymi detekcjami. Zwrócony obiekt będzie listą, każdy element tej listy odpowiada detekcjom dokonanym przez każdą z 3 wyjściowych warstw. Detekcji mamy tyle, na ile komórek warstwa dzieliła obrazek. Pojedyncze detekcje są wektorami o wymiarach (85,), w których:\n",
    "* pierwszy element to współrzędna $x$ położenia środka wykrytego *bounding box*u (w pikselach, mierząc od najbardziej lewego górnego piksela),\n",
    "* drugi element to współrzędna $y$ położenia środka wykrytego *bounding box*u,\n",
    "* trzeci element to względna szerokość *bounding box*u (w postaci ułamka, gdzie 1 to całkowita szerokość analizowanego obrazka),\n",
    "* czwarty element to względna wysokość *bounding box*u, \n",
    "* elementy od 6 do końca to prawdopodobieństwa należenia przez wykryty obiekt do każdej z 80 klas z zestawu danych COCO.\n",
    "\n",
    "\n",
    "Napiszmy funkcję `yolo_predict`, w której zbierzemy otrzymane detekcje, usuniemy te, których model jest mało pewny (tam, gdzie żadne prawdopodobieństwo nie przekracza 0,5), a także przekształcimy je nieco, tak, by ostatecznie otrzymywać listę 6-elementowych wektorów, w których:\n",
    "* pierwszy element to indeks (zgodny z klasami zestawu danych COCO) klasy, do której najprawdopodobniej należy wykryty obiekt,\n",
    "* drugi element to prawdopodobieństwo przynależenia obiektu do tej klasy,\n",
    "* trzeci i czwarty element to współrzędne $x$ i $y$ położenia lewego górnego wierzchołka *bounding box*u,\n",
    "* piąty i szósty element to szerokość i wysokość (nie względne, lecz podane w pikselach) *bounding box*u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict(model, input_img, img_dim, output_layers, classes):\n",
    "    \"\"\"\n",
    "    Funkcja wyzwalająca predykcję modelu YOLO i wyznaczająca parametry odnalezionych obiektów o sensownym znaczeniu. \\n\n",
    "    Argumenty: \\n\n",
    "    - model - model YOLOv3 z OpenCV (obiekt klasy cv2.dnn), już wytrenowany (z konfiguracją z yolov3.cfg i wagami z yolov3.weights), \\n\n",
    "    - input_img - obiekt cv2 zawierający przekształcony obraz do analizy (w kolorach BGR), wczytany za pomocą cv2.imread(), \\n\n",
    "    - img_dim - tuple zawierająca 2 elementy: szerokość i wysokość (w  pikselach) oryginalnego zdjęcia z input_img, \\n\n",
    "    - output_layers - lista zawierająca stringi z nazwami wyjściowych warstw w użytym modelu YOLO z OpenCV, otrzymana w wyniku model.getUnconnectedOutLayers(), \\n\n",
    "    - classes - lista zawierająca stringi z nazwami klas rozpoznawanych obiektów (wg zestawu danych COCO). \\n\n",
    "    Zwraca: objects_detected - lista składająca się z dokonanych przez model YOLO detekcji, \n",
    "        każdy jej element to wektor numpy array, shape=(6,), dtype=float, w którym: \\n\n",
    "        - [0] - indeks klasy c, do której należy wykryty obiekt (wg zestawu danych COCO), \\n\n",
    "        - [1] - prawdopodobieństwo pc, że obiekt należy do tej klasy, \\n\n",
    "        - [2] - pozycja x lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [3] - pozycja y lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [4] - szerokość bounding boxu (bezwględna, w pikselach), \\n\n",
    "        - [5] - wysokość bounding boxu (bezwględna, w pikselach). \\n\n",
    "    \"\"\"\n",
    "    # Przekaż obraz do modelu i odczytaj predykcje z wyjściowych warstw\n",
    "    model.setInput(input_img)\n",
    "    outputs = model.forward(output_layers)\n",
    "    # Przeanalizuj każdą predykcję z osobna\n",
    "    objects_detected = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            # ------------------------------------------- UZUPEŁNIJ KOD ------------------------------------------------\n",
    "            # Odczytaj prawdopodobieństwa przynależenia obiektu do każdej z klas COCO (elementy detection od 6 w górę) i zapisz je jako scores\n",
    "            scores = 0\n",
    "            # Znajdź klasę z najwyższym przypisanym prawdopodobieństwem (classID) oraz to prawdopodobieństwo (confidence)\n",
    "            classID = 0\n",
    "            confidence = 0\n",
    "            # Odrzuć te obiekty, którym przypisano niskie prawdopodobieństwa (poniżej 0,5)\n",
    "            if confidence > 0.5:\n",
    "                bounding_box = np.zeros((6)) # utwórz zmienną, w której będziesz przechowywać informacje o wykrytym obiekcie\n",
    "                # Zapisz indeks właściwej klasy i prawdopodobieństwo jako 2 pierwsze elementy w bounding_box\n",
    "                bounding_box[0] = 0\n",
    "                bounding_box[1] = 0\n",
    "                # Odczytaj (procentowe) wymiary (szerokość i wysokość) bounding boxu (odpowiednio 3. i 4. element detection) \n",
    "                # przemnóż przez rzeczywiste wymiary i zapisz jako 5. i 6. element bounding_box\n",
    "                bounding_box[4] =  0\n",
    "                bounding_box[5] =  0\n",
    "                # Odczytaj położenie x środka bounding-boxu - 1. element detection - i przekształć na położenie lewego górnego wierzchołka:\n",
    "                # predykcję (procentową) przemnóż przez rzeczywisty wymiar obrazka, a od tak otrzymanego położenia środka odejmij połowę wymiaru boxa\n",
    "                # Uwaga - zapisz otrzymaną wartość jako int! umieszczając jako 3. element bounding_box\n",
    "                bounding_box[2] = 0\n",
    "                # Powtórz to samo dla położenia y wierzchołka bounding boxa - 2. element detection - \n",
    "                # (tym razem bierz pod uwagę wysokość obrazka) i zapisz jako 4. element bounding_box\n",
    "                bounding_box[3] = 0\n",
    "                # -------------------------------------------------------------------------------------------------------------\n",
    "                objects_detected.append(bounding_box)\n",
    "                print(\"Wykryto: \"+classes[classID]+\", \"+str(confidence)+\" pewności, (\"+str(bounding_box[2])+\",\"+str(bounding_box[3])+\"), \"+str(bounding_box[4])+\"x\"+str(bounding_box[5]))\n",
    "    return objects_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność napisanego przez siebie kodu - uruchom poniższą komórkę, aby w dosyć intuicyjny sposób zobaczyć, jakich detekcji dokonał model YOLO na analizowanym zdjęciu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zwróć informacje o wykrytych obiektach\n",
    "objects_detected = yolo_predict(\n",
    "    model=yolo, \n",
    "    input_img=input_img, \n",
    "    img_dim=img_dim, \n",
    "    output_layers=output_layers,\n",
    "    classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rysowanie krawędzi *bounding box* naokoło wykrytego obiektu\n",
    "\n",
    "Aby mieć bardziej wizualny podgląd na to, jakie obiekty wykrył nasz model, warto narysować naokoło tych obiektów odpowiadające im *bounding box*y. W bibliotece OpenCV znajduje się dedykowana do tego metoda, lecz my wykorzystamy inną, rysującą na obrazku pojedyncze linie, a nie cały prostokąt od razu, w celu lepszego zrozumienia danych zwracanych przez nasz model w `objects_detected`. Skorzystamy z metody `cv2.line` ([TUTAJ](https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga7078a9fae8c7e7d13d24dac2520ae4a2)), która jako argumentów potrzebuje:\n",
    "* analizowanego obrazu (oryginalnego), wczytanego za pomocą `cv2.imread()`,\n",
    "* położenia ($x,y$) punktów będących początkiem i końcem rysowanej linii (współrzędne te, względem najbardziej lewego górnego piksela, muszą być w formacie int),\n",
    "* koloru (w postaci wektora RGB *shape=(3,)*, int), na jaki ma zostać pokolorowana rysowana linia,\n",
    "* grubości rysowanej linii (w pt, int).\n",
    "\n",
    "<font size=\"2\">Posiłkuj się kodem już napisanym, rysującym jedną z linii. Pamiętaj, że w obecnym stanie nie znamy położenia końca linii - musimy go policzyć, znając szerokość/wysokość bboxa!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(image, objects_detected, colors):\n",
    "    \"\"\"\n",
    "    Funkcja rysująca na analizowanym obrazku boki prostokątów okalających odnalezione obiekty wraz z ich opisem. \\n\n",
    "    Argumenty: \\n\n",
    "    - image - obiekt cv2 zawierający oryginalny obraz do analizy (w kolorach BGR), wczytany za pomocą cv2.imread(), \\n\n",
    "    - objects_detected - lista składająca się z dokonanych przez model YOLO detekcji, \n",
    "        każdy jej element to wektor numpy array, shape=(6,), dtype=float, w którym: \\n\n",
    "        - [0] - indeks klasy c, do której należy wykryty obiekt (wg zestawu danych COCO), \\n\n",
    "        - [1] - prawdopodobieństwo pc, że obiekt należy do tej klasy, \\n\n",
    "        - [2] - pozycja x lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [3] - pozycja y lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [4] - szerokość bounding boxu (bezwględna, w pikselach), \\n\n",
    "        - [5] - wysokość bounding boxu (bezwględna, w pikselach), \\n\n",
    "    - colors - wektor zawierający przypisany każdej klasie kolor RGB, w celu ich odróżniania (numpy array, shape=(num_classes,3), dtype=float). \\n\n",
    "    \"\"\"\n",
    "    for object in objects_detected:\n",
    "        # Narysuj linię od lewego górnego do prawego górnego wierzchołka \n",
    "        cv2.line(img=image, # wskazujemy właściwy obrazek (już wcześniej wczytany przez cv2)\n",
    "                 pt1=(int(object[2]),int(object[3])), # położenie początku linii (w formie tuple) pobieramy z danych o obiekcie\n",
    "                 pt2=(int(object[2]+object[4]), int(object[3])), # położenie końca linii zależy od położenia wierzchołka ORAZ długości bounding boxa\n",
    "                 color=colors[int(object[0])], # z wysolowanych kolorów wybieramy ten, który został przypisany do klasy naszego obiektu\n",
    "                 thickness=3) # grubość linii - taka, by linia była widoczna\n",
    "        # -------------------------------------- UZUPEŁNIJ KOD -------------------------------------------\n",
    "        # Narysuj linię od lewego górnego do lewego dolnego wierzchołka \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Narysuj linię od lewego dolnego do prawego dolnego wierzchołka \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Narysuj linię od prawego górnego do prawego dolnego wierzchołka \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # Opisz bounding box nazwą klasy obiektu oraz oszacowanym prawdopodobieństwem\n",
    "        cv2.putText(img=image, \n",
    "                    text=classes[int(object[0])]+\", \"+str(object[1]),\n",
    "                    org=(int(object[2]),int(object[3])),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=1,\n",
    "                    color=colors[int(object[0])],\n",
    "                    thickness=3)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uruchamiając poniższy kod, zobacz, jakie *bounding box*y otaczające wykryty obiekt stworzył nasz model.\n",
    "<font size=\"2\">Do napisanej funkcji przekazujemy \"głęboką kopię\" naszego zdjęcia, aby rysowane przez nas krawędzie nie były na nim nadpisywane. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl obrazek i wykryte obiekty\n",
    "draw_bounding_boxes(copy.deepcopy(image), objects_detected, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja Non-Max Suppression - usuwanie zbędnych, nadmiarowych *bounding box*ów\n",
    "\n",
    "Zwróć uwagę, że na powyższym zdjęciu, jeden obiekt - pies - został wykryty wielokrotnie. To jest typowy problem występujący podczas pracy z algorytmem YOLO: jeden obiekt może zostać otoczony wieloma prostokątami pochodzącymi z sąsiadujących komórek. Należy zatem wybrać jeden, najbardziej pasujący *bounding box*. W tym celu stosuje się metodę **Non-Max Suppression**: obliczana jest miara Jaccarda (nazywana też *Intersection over Union*, IoU) pomiędzy obszarem zajętym przez *bounding box* o najwyższym prawdopodobieństwie $p_c$ , a wszystkimi innymi przyporządkowanymi do klasy $c$:\n",
    "\\begin{equation*}\n",
    "    \\textrm{IoU} = \\frac{\\textrm{powierzchnia części wspólnej obszarów}}{\\textrm{powierzchnia sumy obszarów}}\n",
    "\\end{equation*}\n",
    "* Jeśli IoU jest wysokie (powyżej 0,5), prawdopodobnie *bounding box*y otaczają ten sam obiekt — usuwane są wtedy wszystkie *bounding box*y oprócz tego z najwyższym prawdopodobieństwem $p_c$.\n",
    "* Jeśli IoU jest niskie, prawdopodobnie porównywane *bounding box*y otaczają dwa różne obiekty tej samej klasy - należy zatem je zostawić.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/Argenni/GUiAO_lab/main/rys/08_nms.png'/>\n",
    "\n",
    "<font size=\"1\">Grafika: baeldung.com</font>\n",
    "</div>\n",
    "\n",
    "Tak, w bibliotece OpenCV znajduje się metoda wykonująca NMS za nas. Nie, nie skorzystamy z niej - dla treningu (a trening czyni przecież mistrza) napiszemy jej własną implementację. \n",
    "\n",
    "Zacznijmy od wyznaczania IoU, tj. stopień tego, jak bardzo dwa *bounding box*y nakładają się na siebie. W tym celu napiszemy specjalną funkcję `compute_iou`, do której przekazywać będziemy dane o bboxach z dwóch wybranych detekcji. Chcemy najpierw obliczyć pole powierzchni pokrywania się bboxów, a następnie powierzchnię zajmowaną przez oba bboxy (pole sumy powierzchni - ale sumy w kontekście zbiorów, a nie arytmetycznej! - jak na powyższym rysunku). \n",
    "\n",
    "<font size=\"2\">Wskazówki: \n",
    "* Pole prostokąta obliczysz w prosty sposób - mnożąc długości jego boków. Z kolei jeśli nie znasz długości boku, możesz je obliczyć, odejmując od większej współrzędnej tą mniejszą!\n",
    "* Przydatne będą takie metody, jak `np.maximum` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.maximum.html)) czy `np.minimum`([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.minimum.html)). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Funkcja służąca do obliczenia IoU - metryki określającej stopień nakładania się dwóch bounding boxów:\n",
    "    IoU = pole części wspólnej dwóch bbox / pole sumy dwóch bbox\n",
    "    Argumenty: bbox1 i bbox2 - wektory numpy array, shape=(6,), dtype=float, z 2 dokonanymi przez model YOLO detekcjami, w których: \\n\n",
    "        - [0] - indeks klasy c, do której należy wykryty obiekt (wg zestawu danych COCO), \\n\n",
    "        - [1] - prawdopodobieństwo pc, że obiekt należy do tej klasy, \\n\n",
    "        - [2] - pozycja x lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [3] - pozycja y lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [4] - szerokość bounding boxu (bezwględna, w pikselach), \\n\n",
    "        - [5] - wysokość bounding boxu (bezwględna, w pikselach). \\n\n",
    "    Zwraca: obliczoną wartość IoU (skalar, float).\n",
    "    \"\"\"\n",
    "    # --------------------------------- UZUPEŁNIJ KOD -----------------------------------\n",
    "    # Wyznacz pole części wspólnej dwóch bounding boxów:\n",
    "    # a) Poszukaj pozycji x lewego górnego wierzchołka części wspólnej \n",
    "    # (największej wartości spośród współrzędnych x lewych górnych wierzchołków dwóch bboxów)\n",
    "    xi1 = 0\n",
    "    # b) Powtórz to samo dla współrzędnej y lewego górnego wierzchołka\n",
    "    yi1 = 0\n",
    "    # c) Poszukaj pozycji x prawego dolnego wierzchołka części wspólnej \n",
    "    # (najmniejszej wartości spośród współrzędnych x prawych dolnych wierzchołków dwóch bboxów - musisz obliczyć oba x)\n",
    "    xi2 = 0\n",
    "    # d) Powtórz to samo dla współrzędnej y prawego dolnego wierzchołka\n",
    "    yi2 = 0\n",
    "    # e) Oblicz szukane pole prostokąta - przemnóż długości boków prostokąta, a zatem różnice pomiędzy współrzędnymi wyliczonymi wcześniej\n",
    "    intersection = 0\n",
    "    # Wyznacz pole sumy obszarów:\n",
    "    # a) Oblicz pole pierwszego bboxa (przemnóż jego szerokość razy wysokość)\n",
    "    box1_area = 0\n",
    "    # b) Oblicz pole drugiego bboxa\n",
    "    box2_area = 0\n",
    "    # c) Oblicz poszukiwane pole sumy, pamiętając z probabilistyki, że suma_A_i_B = A + B - część_wspólna_A_i_B\n",
    "    union = 0\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź, czy poprawnie zaimplementowałeś obliczanie IoU - uruchom kod z poniższej komórki, a jeśli wszystko jest poprawnie napisane, powinienieś w wyniku otrzymać wartość ok. 0.143 ($\\frac{1}{7}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IoU\n",
    "iou = compute_iou((0,0,0,0,4,2),(0,0,2,1,4,2))\n",
    "print(\"Przykładowa wartość obliczonego IoU dla bboxów (0,0,0,0,4,2),(0,0,2,1,4,2): \" + str(iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umiejąc liczyc IoU, napisz teraz fragment kodu (funkcję `perform_nms`), który realizuje całość algorytmu *Non-Max Suppression* zgodnie z powyższym opisem. Chcemy, aby w wyniku tej funkcji usunięte zostały te elementy listy `objects_detected`, które mają zbyt wysoki (powyżej 0,5) IoU z detekcją z najwyższym prawdopodobieństwem. \n",
    "\n",
    "<font size=\"2\">Wskazówka: przydatne będą takie metody, jak `np.argsort` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)), `np.flip`([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.flip.html)), `np.where` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.where.html)), `np.delete` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.delete.html)). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_nms(objects_detected):\n",
    "    \"\"\"\n",
    "    Funkcja redukująca detekcje dotyczące tego samego obiektu. \\n\n",
    "    Argument: objects_detected - lista składająca się z dokonanych przez model YOLO detekcji, \n",
    "        każdy jej element to wektor numpy array, shape=(6,), dtype=float, w którym: \\n\n",
    "        - [0] - indeks klasy c, do której należy wykryty obiekt (wg zestawu danych COCO), \\n\n",
    "        - [1] - prawdopodobieństwo pc, że obiekt należy do tej klasy, \\n\n",
    "        - [2] - pozycja x lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [3] - pozycja y lewego górnego wierzchołka bounding boxu (w pikselach, względem najbardziej lewego górnego piksela), \\n\n",
    "        - [4] - szerokość bounding boxu (bezwględna, w pikselach), \\n\n",
    "        - [5] - wysokość bounding boxu (bezwględna, w pikselach). \\n\n",
    "    Zwraca: objects_detected_supp - lista składająca się z przefiltrowanych detekcji, budowa jw.\n",
    "    \"\"\"\n",
    "    objects_detected = np.array(objects_detected)\n",
    "    objects_detected_supp = []\n",
    "    # ---------------------------------------- UZUPEŁNIJ KOD -----------------------------------------\n",
    "    # Posortuj detekcje od najwyższego do najniższego prawodpodobieństwa (kolumna 2)\n",
    "    indices = 0\n",
    "    indices = 0\n",
    "    objects_detected = 0\n",
    "    for c in set(objects_detected[:,0]):\n",
    "        # Znajdź obiekty danej klasy c i zapisz je jako objects_class\n",
    "        objects_class = 0\n",
    "        while (objects_class.shape[0]>0): # Dopóki są jakieś obiekty tej klasy do analizowania...\n",
    "            # Wybierz obiekt najbardziej pewny - pierwszy z posortowanej listy\n",
    "            currently_most_confident = 0\n",
    "            # Zachowaj go w objects_detected_supp\n",
    "            \n",
    "            # Usuń go z tablicy analizowanych obiektów objects_class, by go powtórnie nie analizować\n",
    "            objects_class = 0\n",
    "            indices_to_delete = []\n",
    "            for i in range(objects_class.shape[0]):\n",
    "                # Oblicz jego IoU z innymi obiektami tej samej klasy (wywołaj compute_iou)\n",
    "                iou = 0\n",
    "                # Jeśli jakiś inny obiekt zbyt mocno się z nim pokrywa, zapisz jego indeks w indices_to_delete\n",
    "                if iou > 0.5: indices_to_delete = 0\n",
    "            # Wyrzuć nakładające się obiekty (z indiced_to_delete) z tablicy objects_class\n",
    "            objects_class = 0\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    return objects_detected_supp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli Twoja implementacja jest bez błędów, po uruchomieniu kodu z poniższej komórki powinienieś zobaczyć to samo zdjęcie psa, lecz otoczone już tylko jedną - najlepszą - ramką! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usuń zbędne bounding boxy i wyświetl obrazek z detekcjami ponownie\n",
    "objects_detected_supp = perform_nms(objects_detected)\n",
    "draw_bounding_boxes(copy.deepcopy(image), objects_detected_supp, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brawo, umiesz już dokonywać detekcji obiektów z użyciem algorytmu YOLO! Możesz teraz poeksperymentować, dokonując analizy dowolnego zdjęcia.\n",
    "\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "1. Na czym polega problem detekcji obiektów i z jakich składowych problemów się składa?\n",
    "2. Opisz, jak działa algorytm YOLO.\n",
    "3. Na czym polega i po co stosowana jest metoda *Non-Max Suppression*?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
